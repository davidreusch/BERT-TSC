{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "from transformers import BertTokenizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data_pth = 'Data/'\n",
    "    train_pth = data_pth + 'train.csv'\n",
    "    test_pth = data_pth + 'test.csv'\n",
    "    train = pd.read_csv(train_pth)\n",
    "    test = pd.read_csv(test_pth)\n",
    "    # print(test)\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEGACY\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1) # TODO anpassen\n",
    "\n",
    "    def forward(self, Q:torch.Tensor, K:torch.Tensor, V:torch.Tensor):\n",
    "        assert(Q.shape[-1] == K.shape[-2])\n",
    "        assert(K.shape[-1] == V.shape[-2])\n",
    "        d_k = Q.shape[-1]\n",
    "        # matmul is done between last 2 dimension, the rest is batch!\n",
    "        Z = Q @ K.T  / d_k **.5\n",
    "        A = self.softmax(Z) \n",
    "        res = A @ V\n",
    "        return res\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model:int, d_k:int, d_v:int) -> None:\n",
    "        super().__init__()\n",
    "        W_Q = torch.randn(d_model, d_k, dtype=torch.float32, requires_grad=True)\n",
    "        W_K = torch.randn(d_model, d_k, dtype=torch.float32, requires_grad=True)\n",
    "        W_V = torch.randn(d_model, d_v, dtype=torch.float32, requires_grad=True)\n",
    "        self.W_Q = nn.Parameter(W_Q) \n",
    "        self.W_K = nn.Parameter(W_K) \n",
    "        self.W_V = nn.Parameter(W_V) \n",
    "\n",
    "        self.attention = Attention()\n",
    "\n",
    "    def forward(self, Q:torch.Tensor, K:torch.Tensor, V:torch.Tensor):\n",
    "        Q_proj = Q @ self.W_Q\n",
    "        K_proj = K @ self.W_K\n",
    "        V_proj = V @ self.W_V\n",
    "        return self.attention(Q_proj, K_proj, V_proj)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, d_k: int, d_v: int, p_dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        attention_heads = [AttentionHead(d_model=d_model, d_k = d_k, d_v = d_v) for _ in range(num_heads)]\n",
    "        self.attention_heads = nn.ModuleList(attention_heads)\n",
    "        self.W_O = torch.randn(num_heads*d_v, d_model, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    def forward(self, Q:torch.Tensor, K:torch.Tensor, V:torch.Tensor):\n",
    "        heads = torch.cat([attention_head(Q, K, V) for attention_head in self.attention_heads])\n",
    "        return heads @ self.W_O\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        return self.linear2(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads = num_heads, d_model = d_model, d_k = d_model, d_v = d_model)\n",
    "        self.batchnorm1 = nn.BatchNorm1d()\n",
    "        self.ffn = FFN()\n",
    "        self.batchnorm2 = nn.BatchNorm1d()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, input_seq: torch.Tensor):\n",
    "        assert(input_seq.shape[1] == self.d_model)\n",
    "        x = self.mha(input_seq, input_seq, input_seq)\n",
    "        x = self.batchnorm1(x) + input_seq\n",
    "        y = self.ffn(x)\n",
    "        y = self.batchnorm2(y) + x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_135b8_row0_col0, #T_135b8_row0_col1, #T_135b8_row0_col2, #T_135b8_row0_col3, #T_135b8_row0_col4, #T_135b8_row0_col5, #T_135b8_row0_col6, #T_135b8_row1_col0, #T_135b8_row1_col1, #T_135b8_row1_col2, #T_135b8_row1_col3, #T_135b8_row1_col4, #T_135b8_row1_col5, #T_135b8_row1_col6, #T_135b8_row2_col0, #T_135b8_row2_col1, #T_135b8_row2_col2, #T_135b8_row2_col3, #T_135b8_row2_col4, #T_135b8_row2_col5, #T_135b8_row2_col6, #T_135b8_row3_col0, #T_135b8_row3_col1, #T_135b8_row3_col2, #T_135b8_row3_col3, #T_135b8_row3_col4, #T_135b8_row3_col5, #T_135b8_row3_col6, #T_135b8_row4_col0, #T_135b8_row4_col1, #T_135b8_row4_col2, #T_135b8_row4_col3, #T_135b8_row4_col4, #T_135b8_row4_col5, #T_135b8_row4_col6 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_135b8_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >comment_text</th>\n",
       "      <th class=\"col_heading level0 col1\" >toxic</th>\n",
       "      <th class=\"col_heading level0 col2\" >severe_toxic</th>\n",
       "      <th class=\"col_heading level0 col3\" >obscene</th>\n",
       "      <th class=\"col_heading level0 col4\" >threat</th>\n",
       "      <th class=\"col_heading level0 col5\" >insult</th>\n",
       "      <th class=\"col_heading level0 col6\" >identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_135b8_level0_row0\" class=\"row_heading level0 row0\" >82115</th>\n",
       "      <td id=\"T_135b8_row0_col0\" class=\"data row0 col0\" >Bot error \n",
       "\n",
       "OrphanBot applied an incorrect tag to an image. See  — the image had tag  already on it; the bot tagged it with .  Why is the bot misidentifying licensed images? —   \n",
       "\n",
       "The image has only the simple assertion that it is in the public domain.  It needs source information so that it's possible for this to be verified.</td>\n",
       "      <td id=\"T_135b8_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_135b8_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_135b8_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_135b8_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "      <td id=\"T_135b8_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_135b8_row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_135b8_level0_row1\" class=\"row_heading level0 row1\" >38292</th>\n",
       "      <td id=\"T_135b8_row1_col0\" class=\"data row1 col0\" >\"\n",
       "\n",
       " Reply \n",
       "\n",
       "Thank you for your note. Apart from the issue of the external link mentioned above, the text you refer to was removed because it made an unreferenced claim (\"\"One of the most important museums in Barcelona\"\") which needs a reliable and independent source, and also because it makes specific mention of the contractor, which is a) possibly promotional and b) possibly not relevant to the article on Barcelona. Regards,   \"</td>\n",
       "      <td id=\"T_135b8_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_135b8_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_135b8_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_135b8_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_135b8_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_135b8_row1_col6\" class=\"data row1 col6\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_135b8_level0_row2\" class=\"row_heading level0 row2\" >28694</th>\n",
       "      <td id=\"T_135b8_row2_col0\" class=\"data row2 col0\" >\"\n",
       " Image:PA_writing.jpg \n",
       "Thanks for uploading Image:PA_writing.jpg. I notice the 'image' page specifies that the image is being used under fair use, but its use in Wikipedia articles fails our first fair use criterion in that it illustrates a subject for which a freely licensed image could reasonably be found or created that provides substantially the same information. If you believe this image is not replaceable, please:\n",
       "\n",
       " Go to the image description page and edit it to add {{Replaceable fair use disputed}}, without deleting the original Replaceable fair use template.\n",
       " On the image discussion page, write the reason why this image is not replaceable at all.\n",
       "\n",
       "Alternatively, you can also choose to replace the fair use image by finding a freely licensed image of its subject, requesting that the copyright holder release this (or a similar) image under a free license, or by taking a picture of it yourself.\n",
       "\n",
       "If you have uploaded other fair use media, consider checking that you have specified how these images fully satisfy our fair use criteria. You can find a list of 'image' pages you have edited by clicking on [ this link]. Note that any fair use images which are replaceable by free-licensed alternatives will be deleted one week after they have been uploaded, as described on criteria for speedy deletion.  If you have any questions please ask them at the Media copyright questions page. Thank you. ≈talk \"</td>\n",
       "      <td id=\"T_135b8_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_135b8_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_135b8_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_135b8_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "      <td id=\"T_135b8_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_135b8_row2_col6\" class=\"data row2 col6\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_135b8_level0_row3\" class=\"row_heading level0 row3\" >121889</th>\n",
       "      <td id=\"T_135b8_row3_col0\" class=\"data row3 col0\" >The distinction I would draw is whether the content is published and accepted.  Wikipedia:No original research#What_is_excluded.3F gives a list of seven criteria for determining whether or not something is original research ... but really what it boils down to is whether or not you can find other secondary sources making the same claim.  I had never heard of this tomb before this evening so obviously I don't know enough about it to say what is and is not accepted science in this respect.  ;)  From what you are saying, it sounds like there are two questions - (1) is it appropriate to discuss the symbolism findings in the article and (2) if so, is it appropriate to cite your work.  In the case of the former, it depends on whether or not this has become an accepted finding.  Is it still a theory or has it gained acceptance?  Have other authors picked up on it and do they cite it as accepted truth or as a new theory?  The primary reason for the original research policy is that there are constantly new theories coming out about everything - Jimbo (the founder of Wikipedia) originally mentioned physics, but I think it applies to any field.  Some theories are good for a term paper or thesis and never see the light of day again.  Others become accepted truth over time.  Wikipedia is not the place for giving a sounding board to new ideas and theories - we wait until they are accepted.  The second question is whether, if this content is to be included, it would be appropriate to cite your book.  I believe, as I said before, that you (personally) should not add it, but should let someone else do it - that's just basic journalistic integrity - we as individual editors don't promote our own stuff.  Blogs are almost never considered appropriate ... so I really don't think having a link to your blog would be a good idea.  Sources on Wikipedia should be peer-reviewed journals, newspapers, etc, where more than one person is responsible for the content.  Blogs are generally a bad thing.  I hope all this helps ... I know I've rambled a bit. (As a quick side note, please sign messages on talk pages using four tildes - ~~~~ - it will automatically turn into a time/date stamp with your name ... it makes conversations easier to follow.)</td>\n",
       "      <td id=\"T_135b8_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_135b8_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_135b8_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "      <td id=\"T_135b8_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "      <td id=\"T_135b8_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_135b8_row3_col6\" class=\"data row3 col6\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_135b8_level0_row4\" class=\"row_heading level0 row4\" >75586</th>\n",
       "      <td id=\"T_135b8_row4_col0\" class=\"data row4 col0\" >Go fuck yourself, you piece of shit.</td>\n",
       "      <td id=\"T_135b8_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "      <td id=\"T_135b8_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_135b8_row4_col3\" class=\"data row4 col3\" >1</td>\n",
       "      <td id=\"T_135b8_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "      <td id=\"T_135b8_row4_col5\" class=\"data row4 col5\" >1</td>\n",
       "      <td id=\"T_135b8_row4_col6\" class=\"data row4 col6\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6d5eae7be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def train_val_split(df: pd.DataFrame, train_frac):\n",
    "    out = df.copy()\n",
    "    ind = df.index\n",
    "    perm = np.random.permutation(ind)\n",
    "    split = int(len(perm) * train_frac)\n",
    "    perm_train, perm_val = perm[:split], perm[split:]\n",
    "    train, val  = out.iloc[perm_train], out.iloc[perm_val]\n",
    "    return train, val\n",
    "\n",
    "# print(pd.__version__)\n",
    "# print(train_set)\n",
    "train, val = train_val_split(train_set, train_frac=0.8)\n",
    "\n",
    "# print((train))\n",
    "# print((val))\n",
    "assert(len(train) + len(val) == len(train_set))\n",
    "val_to_show = val.drop(['id'], axis=1).iloc[:5]\n",
    "train_to_show = train.drop(['id'], axis=1).iloc[:5]\n",
    "# train_to_show = train.iloc[:5]\n",
    "# val_to_show.style.format(formatter={'comment_text': '{:<10}'})\n",
    "# display(HTML(val_to_show.to_html()))\n",
    "# display(Markdown(val_to_show.to_markdown()))\n",
    "# df_latex = val_to_show.style.set_properties(**{'text-align': 'left'}).to_latex()\n",
    "# print(df_latex)\n",
    "# display(Latex('$' + df_latex + '$'))\n",
    "val_to_show.style.set_properties(**{'text-align': 'left'})\n",
    "train_to_show.style.set_properties(**{'text-align': 'left'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "cfg = {'num_encoder_blocks': 12,\n",
    "       'num_attention_heads': 12,\n",
    "       'd_model': 768,\n",
    "       'vocab_size': 30522,\n",
    "       'max_seq_len': 512, \n",
    "       'p_dropout': 0.1, \n",
    "       'batchsize': 16,\n",
    "       'num_target_categories': 6\n",
    "       }\n",
    "cfg = Namespace(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  9326,  1204,  ...,     0,     0,     0],\n",
      "        [  101,   107, 20777,  ...,     0,     0,     0],\n",
      "        [  101,   107, 15065,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   155, 10069,  ...,     0,     0,     0],\n",
      "        [  101,  7102,  1146,  ...,     0,     0,     0],\n",
      "        [  101,  1130,   124,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "def make_batches(dataset, tokenizer, batch_size):\n",
    "    label_tags = [\"toxic\", \"severe_toxic\"\t,\"obscene\"\t,\"threat\"\t,\"insult\"\t,\"identity_hate\"]\n",
    "    batches = []\n",
    "    for b in range(0, len(dataset), batch_size):\n",
    "        ds_batch = dataset.iloc[b:b+batch_size]\n",
    "        # print(ds_batch)\n",
    "        if len(ds_batch) == 0: break\n",
    "        sentences = ds_batch['comment_text'].to_list()\n",
    "        # max_len = max(len(s) for s in sentences)\n",
    "        token_dict = tokenizer(sentences, return_tensors='pt', padding='longest', truncation=True)\n",
    "        labels = ds_batch[label_tags].to_numpy(dtype=int)\n",
    "        token_dict['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        # print(token_dict)\n",
    "        batches.append(token_dict)\n",
    "    return batches\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "train_batches = make_batches(train.iloc[:256], tokenizer, cfg.batchsize)\n",
    "\n",
    "print(train_batches[0])\n",
    "# for i in range(5):\n",
    "    # print({k: v.shape for (k,v) in train_batches[i].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(cfg.d_model, cfg.d_model)\n",
    "        self.key = nn.Linear(cfg.d_model, cfg.d_model)\n",
    "        self.value = nn.Linear(cfg.d_model, cfg.d_model)\n",
    "        self.dropout = nn.Dropout(cfg.p_dropout)\n",
    "        self.num_attention_heads = cfg.num_attention_heads\n",
    "\n",
    "    def forward(self, seq:torch.Tensor):\n",
    "        # seq.shape = (batchsize, seq_len, d_model)\n",
    "        batchsize, seq_len, d_model = seq.shape\n",
    "        assert(d_model % self.num_attention_heads == 0)\n",
    "        d_v = d_model // self.num_attention_heads\n",
    "        querys_proj = self.query(seq).view(batchsize, seq_len, self.num_attention_heads, d_v)\n",
    "        querys_proj = querys_proj.transpose(1, 2)\n",
    "        keys_proj = self.key(seq).view(batchsize, seq_len, self.num_attention_heads, d_v)\n",
    "        keys_proj = keys_proj.transpose(1, 2)\n",
    "        values_proj = self.value(seq).view(batchsize, seq_len, self.num_attention_heads, d_v)\n",
    "        values_proj = values_proj.transpose(1, 2)\n",
    "\n",
    "        #   (batchsize, num_attention_heads, seq_len, d_v) x (batchsize, num_attention_heads, d_v, seq_len)\n",
    "        # = (batchsize, num_attention_heads, seq_len, seq_len)\n",
    "        Z = querys_proj @ keys_proj.transpose(-1, -2) / d_v**.5\n",
    "        p_attention = nn.Softmax(dim=-1)(Z)\n",
    "        #   (batchsize, num_attention_heads, seq_len, seq_len) x (batchsize, num_attention_heads, seq_len, d_v)\n",
    "        # = (batchsize, num_attention_heads, seq_len, d_v)\n",
    "        attention_output = p_attention @ values_proj\n",
    "\n",
    "        # concatenate the heads along the last axis by reshaping the output to (batchsize, seq_len, d_model) again\n",
    "        attention_output = attention_output.transpose(1, 2).reshape(batchsize, seq_len, d_v * self.num_attention_heads)\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.d_model, cfg.d_model)\n",
    "        self.LayerNorm = nn.LayerNorm((cfg.d_model,), eps=1e-12, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(cfg.p_dropout)\n",
    "    \n",
    "    def forward(self, attention_output):\n",
    "        # attention_output.shape = (batchsize, seq_len, d_model)\n",
    "        # in which order to apply these things? see below\n",
    "        return self.dropout(self.LayerNorm(self.dense(attention_output))) + attention_output\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        # self.attention = BertSelfAttention(cfg)\n",
    "        self.self = BertSelfAttention(cfg)\n",
    "        self.output = BertSelfOutput(cfg)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        attention_output = self.self(seq)\n",
    "        return self.output(attention_output)\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.d_model, cfg.d_model * 4)\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, seq):\n",
    "        return self.intermediate_act_fn(self.dense(seq))\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.d_model * 4, cfg.d_model)\n",
    "        self.LayerNorm = nn.LayerNorm((cfg.d_model,), eps=1e-12, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(cfg.p_dropout)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # in which order to apply dropout, layernorm and residual connection?\n",
    "        # in Vaswani et al it is layernorm(x + sublayer(x))\n",
    "        # the order here is from annotated transformer\n",
    "        return self.dropout(self.LayerNorm(self.dense(seq)))\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(cfg)\n",
    "        self.intermediate = BertIntermediate(cfg)\n",
    "        self.output = BertOutput(cfg)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        attention_output = self.attention(seq)\n",
    "        x = self.intermediate(attention_output)\n",
    "        x = self.output(x)\n",
    "        return x + attention_output\n",
    "        \n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        encoder_stack = [BertLayer(cfg) for _ in range(cfg.num_encoder_blocks)]\n",
    "        self.layer = nn.ModuleList(encoder_stack)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor):\n",
    "        encoder_stack = nn.Sequential(*self.layer)\n",
    "        return encoder_stack(seq)\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.position_embeddings = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
    "        self.token_type_embeddings = nn.Embedding(2, cfg.d_model) # only beginning of sentence token and other tokens\n",
    "        self.LayerNorm = nn.LayerNorm((cfg.d_model,), eps=1e-12, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(cfg.p_dropout)\n",
    "        self.register_buffer(\"position_ids\", torch.arange(cfg.max_seq_len).expand((1, -1)))\n",
    "        self.device = cfg.device\n",
    "    \n",
    "    # def forward(self, input_ids, token_type_ids):\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        batchsize, seq_len = input_ids.shape\n",
    "        position_ids = torch.stack([torch.arange(0, seq_len, dtype=torch.long, device=self.device)] * batchsize)\n",
    "        input_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "        # print(f\"{input_embeds.shape=}\")\n",
    "        # print(f\"{token_type_embeds.shape=}\")\n",
    "        # print(f\"{position_embeds.shape=}\")\n",
    "        embeds = input_embeds + position_embeds + token_type_embeds\n",
    "        embeds = self.LayerNorm(input_embeds)\n",
    "        embeds = self.dropout(embeds)\n",
    "        return embeds\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.d_model, cfg.d_model)\n",
    "\n",
    "class MyBertModel(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings(cfg)\n",
    "        self.encoder = BertEncoder(cfg)\n",
    "        self.pooler = BertPooler(cfg)\n",
    "\n",
    "    def forward(self, input_ids:torch.Tensor, token_type_ids:torch.Tensor):\n",
    "        embeds = self.embeddings(input_ids, token_type_ids)\n",
    "        encoder_output = self.encoder(embeds)\n",
    "        return encoder_output\n",
    "\n",
    "\n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(cfg.d_model, cfg.d_model // 2)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.dense2 = nn.Linear(cfg.d_model // 2, cfg.num_target_categories)\n",
    "        # self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(cfg.p_dropout)\n",
    "    \n",
    "    def forward(self, encoder_output:torch.Tensor):\n",
    "        # encoder_output.shape: (batchsize, seq_len, d_model)\n",
    "        x = self.dense1(encoder_output[:, 0]) # use only hidden state corresponding to start of sequence token for classification\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ToxicSentimentClassificationModel(nn.Module):\n",
    "    def __init__(self, cfg, state_dict) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = MyBertModel(cfg)\n",
    "        self.backbone.load_state_dict(state_dict)\n",
    "        self.output_layer = OutputLayer(cfg)\n",
    "    \n",
    "    def forward(self, input_ids:torch.Tensor, token_type_ids:torch.Tensor, **kwargs):\n",
    "        backbone_output = self.backbone(input_ids, token_type_ids)\n",
    "        return self.output_layer(backbone_output)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import BertModel\n",
    "\n",
    "device = torch.device(\"cuda\") if False and torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "cfg.device = device\n",
    "pretrained_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "pretrained_state_dict = pretrained_model.state_dict()\n",
    "# pprint({k: v.shape for (k, v) in pre_state_dict.items()})\n",
    "# pre_state_dict_adapted = {'backbone.' + k: v for (k, v) in pre_state_dict.items()}\n",
    "# pprint({k: v.shape for (k, v) in pre_state_dict_adapted.items()})\n",
    "model = ToxicSentimentClassificationModel(cfg, pretrained_state_dict)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/2test/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loss: 0.7023908495903015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/48 [00:17<13:53, 17.74s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33280/2243616337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# print(f\"{outputs.shape=}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# print(f\"{batch['labels'].shape=}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33280/663134821.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mbackbone_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33280/663134821.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33280/663134821.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mencoder_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoder_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33280/663134821.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33280/663134821.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33280/663134821.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0md_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mquerys_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mquerys_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquerys_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mkeys_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2test/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_batches)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for batch in train_batches:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        # print(f\"{outputs.shape=}\")\n",
    "        # print(f\"{batch['labels'].shape=}\")\n",
    "        loss = loss_fn(outputs, batch['labels'])\n",
    "        # print(f\"{loss.item()=}\")\n",
    "        print('loss:', loss.item())\n",
    "        # loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('2test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eeb8e5f5c66ae994633f3a9739a97020275c72fbf5dcec4486881264c8f56ca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
